# Video-Search-Extract

Collecting training data for computer vision and robotics can be challenging in scenarios that require specialized equipment, such as unique camera setups, filters, and lighting. For example, capturing clear welding pool footage demands specific optical configurations that standard recording methods cannot provide, resulting in data scarcity for these applications.

Some of this data is available in public videos, so this project aims to collect, analyze, and extract video data from those specialized optical scenarios, with a focus on welding pool footage and other hard-to-capture industrial processes.


## Current Functionality, Next Steps, and Findings

Currently uses YouTube-DLP to batch download videos from personally identified and curated sources.  But when it comes to longer videos, it becomes necessary to find and extract the specifically relevant frames and moments from the video.  This is challenging because even audio and transcriptions are not reliable indicators of what is being displayed on screen.  So the next steps are to use VLMs to perform video understanding, specifically specific timestamp and moment extraction.  

However, a spike into this topic shows that there are no plug-and-play tools yet that expose this kind of fine-grained timestamp localization in a way that's easy to use, only academic approaches.

The core problems are: how to find precise moments without overloading memory or sacrificing speed, especially in long videos. Models need to balance detail (high frame/audio sampling) with efficiency (not storing or computing too much). Some use smart ways to keep just the important parts of the video, and others divide the work between modules.  

Others use joint processing of vision, audio, text signals to give better context, but that increases complexity and training instability.  Then you have tradeoffs between generalized models that work across many video tasks vs ones that provide higher accuracy for a single task.  


A few systems mimic human memory by continuously watching, remembering only what matters, and forgetting the rest. Others take their time by looking at the whole video offline to make better predictions. Audio and subtitles are often helpful, but most tools still underuse them.

Most approaches are either VLMs, or specialized, task-specific architectures that focus on timestamp localization or moment retrieval.

## Getting Started

### Installation

1. **Make the script executable:**
   ```bash
   chmod +x yt-download.sh
   ```

2. **Add to PATH:**
   
   **For Bash:**
   ```bash
   echo 'export PATH="/Users/<username and path>/Video-Search-Extract:$PATH"' >> ~/.bashrc
   source ~/.bashrc
   ```
   
   **For Zsh:**
   ```bash
   echo 'export PATH="/Users/<username and path>/Video-Search-Extract:$PATH"' >> ~/.zshrc
   source ~/.zshrc
   ```

### ðŸ“¥ Usage

1. **Prepare Links File**
   
   Create a text file (e.g., `links.txt`) with YouTube URLs:
   ```
   https://youtu.be/video1 [Optional Description]
   https://youtube.com/watch?v=video2 [Another Description]
   ```

2. **Run the Script**
   
   **Standard workflow (with metadata):**
   ```bash
   yt-download.sh links.txt
   ```
   
   **H.264 conversion workflow (recommended for analysis):**
   ```bash
   yt-download.sh --h264 links.txt
   ```
   
   **Get help:**
   ```bash
   yt-download.sh --help
   ```

The script will prompt you for download directory preferences and handle file conflicts automatically.

### Workflow Options

- **Standard Mode**: Downloads with embedded metadata, subtitles, and chapters
- **H.264 Mode** (`--h264`): Downloads best video quality, converts to H.264 MP4 with CRF 23, removes audio and metadata for clean analysis files

## Contributing

This project is focused on specialized video data collection and analysis. Contributions related to:
- Vision Language Model integration
- Video processing optimization
- Specialized optical content detection
- Industrial process video analysis

are particularly welcome.

## License

*[To be determined]*
